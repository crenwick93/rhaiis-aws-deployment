[Unit]
Description=RHAIIS Service
After=network.target

[Service]
Type=simple
Environment="HUGGING_FACE_HUB_TOKEN={{ setup_rhaiis_on_rhel_hf_token }}"
Environment="HF_HUB_OFFLINE=0"
User={{ student_name | default('ec2-user') }}
ExecStart=/bin/bash -c '/usr/bin/podman run --rm -it --device nvidia.com/gpu=all -p {{ setup_rhaiis_on_rhel_vllm_port }}:8000 \
    --ipc=host \
    --env "HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}" \
    --env "HF_HUB_OFFLINE=${HF_HUB_OFFLINE}" \
    --env "HF_HUB_ENABLE_HF_TRANSFER=1" \
    -v {{ setup_rhaiis_on_rhel_vllm_cache_dir }}:/opt/app-root/src/.cache \
    -v /opt/rhaiis/chat_templates:/chat_templates:ro \
    --name={{ setup_rhaiis_on_rhel_vllm_container_name }} \
    {{ setup_rhaiis_on_rhel_vllm_container_image }} \
    --tensor-parallel-size {{ setup_rhaiis_on_rhel_vllm_tensor_parallel_size }} \
    --api_key {{ setup_rhaiis_on_rhel_api_token }} \
    --max-model-len {{ setup_rhaiis_on_rhel_vllm_max_model_len }} \
    --enforce-eager \
    --model {{ setup_rhaiis_on_rhel_vllm_model }}
    --enable-auto-tool-choice \
    --tool-call-parser llama3_json \
    --chat-template /chat_templates/tool_chat_template_llama3.1_json.jinja \
    >> /tmp/rhaiis.log 2>&1'
ExecStop=/usr/bin/podman stop {{ setup_rhaiis_on_rhel_vllm_container_name }}
Restart=on-failure
RestartSec=5s

[Install]
WantedBy=multi-user.target
