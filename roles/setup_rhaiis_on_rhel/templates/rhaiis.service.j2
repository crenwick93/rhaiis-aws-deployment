[Unit]
Description=RHAIIS Service
After=network.target

[Service]
Type=simple
Environment="HUGGING_FACE_HUB_TOKEN={{ setup_rhaiis_on_rhel_hf_token }}"
Environment="HF_HUB_OFFLINE=0"
Environment="HF_HUB_ENABLE_HF_TRANSFER=1"
Environment="VLLM_ATTENTION_BACKEND=XFORMERS"
Environment="PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:256"
ExecStartPre=/usr/bin/nvidia-modprobe -u -c=0
User={{ student_name | default('ec2-user') }}
ExecStart=/bin/bash -c '/usr/bin/podman run --rm --device nvidia.com/gpu=all -p {{ setup_rhaiis_on_rhel_vllm_port }}:8000 \
  --ipc=host \
  -v {{ setup_rhaiis_on_rhel_vllm_cache_dir }}:/home/vllm/.cache \
  --env HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN} \
  --env HF_HUB_OFFLINE=${HF_HUB_OFFLINE} \
  --env HF_HUB_ENABLE_HF_TRANSFER=${HF_HUB_ENABLE_HF_TRANSFER} \
  --env VLLM_ATTENTION_BACKEND=${VLLM_ATTENTION_BACKEND} \
  --env PYTORCH_CUDA_ALLOC_CONF=${PYTORCH_CUDA_ALLOC_CONF} \
  --name={{ setup_rhaiis_on_rhel_vllm_container_name }} \
  {{ setup_rhaiis_on_rhel_vllm_container_image }} \
  --model RedHatAI/Llama-3.2-11B-Vision-Instruct-quantized.w4a16 \
  --api-key {{ setup_rhaiis_on_rhel_api_token }} \
  --tensor-parallel-size 1 \
  --dtype float16 \
  --max-model-len 8192 \
  --max-seq-len-to-capture 4096 \
  --max-num-seqs 1 \
  --gpu-memory-utilization 0.86 \
  --swap-space 8 \
  --enforce-eager \
  --disable-log-stats \
  --enable-auto-tool-choice \
  --tool-call-parser pythonic \
  >> /tmp/rhaiis.log 2>&1'
ExecStop=/usr/bin/podman stop {{ setup_rhaiis_on_rhel_vllm_container_name }}
Restart=on-failure
RestartSec=5s

[Install]
WantedBy=multi-user.target
